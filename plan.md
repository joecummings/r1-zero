 Implementing Packing for GRPO Pipeline

## Goals

1. Implement dynamic sequence packing within both `RefActor` and `PyTorchActorModel` to replace static padding/concatenation for improved efficiency (memory and computation).
2. Modify data structures and flow (`SyncLLMCollector` -> `RefActor` -> `ReplayBuffer` -> `PyTorchActorModel`) to support this distributed packing logic, with the replay buffer storing unpacked, processed sequence data.
3. Ensure correctness of logprob calculation, reward computation, and advantage mapping.
4. Maintain compatibility with GRPO loss calculation which operates on packed batches generated dynamically by `PyTorchActorModel`.

## Key Considerations

- **Distributed Packing:** `RefActor` receives raw, unpacked sequences (`VLLMOutput`) and packs them dynamically based on `max_seq_len` for its processing (ref logprobs, rewards, advantages). `PyTorchActorModel` samples unpacked `TrajectoryData` from the `replay_buffer` and packs it dynamically based on `max_seq_len` for its training step (policy forward pass, loss).
- **Batch Concept:** The `batch_size` config parameter for the `replay_buffer` dictates how many individual `TrajectoryData` instances are sampled by `PyTorchActorModel` *before* attempting to pack them based on `max_seq_len`. The number of sequences in a packed batch (`PackedTrajectory`) can vary depending on sequence lengths.
- **Raw Data Transfer:** `SyncLLMCollector` provides raw (unpadded) prompts and responses via `VLLMOutput`.
- **Replay Buffer Content:** The `ReplayBuffer` stores processed information for *individual sequences* using the `TrajectoryData` TensorClass, in an unpacked format. It concatenates the lists within the TensorClass fields upon `extend`.
- **Masking is Key:** Rely on masks (`response_mask`, `sample_mask`, `attention_mask`) generated *during packing* and padded to `max_seq_len` within actors for operations requiring alignment (attention, loss calculation, data extraction).
- **Prompt/Response Lengths:** Essential for packing and mask creation. Must be tracked for each sequence.
- **Loss Function:** The `GRPOWithChunkedOutputLoss` consumes a packed batch generated by `PyTorchActorModel`. It uses `response_mask` (passed as `padding_masks`) to correctly calculate the loss only over valid response tokens, ignoring padding. The `targets` tensor passed to the loss must also be masked appropriately (e.g., using `ignore_index`) for padding and potentially the final response token.
- **Padding:** Packed batches are padded to `max_seq_len`. Masks and operations must correctly handle these padded regions.
- **TensorClass Usage:** `TrajectoryData`, `VLLMOutput`, and `PackedTrajectory` will be implemented as `TensorClass`. `ReplayBuffer` stores and samples `TrajectoryData`. `PackedTrajectory` is a transient structure created by actors.

## Data Structures

### 1. `VLLMOutput` (`TensorClass`)

- Purpose: Transfer raw data from `SyncLLMCollector` to `RefActor` queue. Contains *unpadded* sequences and grouping info.
- Fields:
  - `prompts`: `List[torch.Tensor]` # List of prompt token tensors (unpadded)
  - `responses`: `List[torch.Tensor]` # List of response token tensors (unpadded)
  - `logprobs`: `List[torch.Tensor]` # List of response logprob tensors (matching `responses`, unpadded)
  - `answers`: `List[str]` # Ground truth answers (len = num_sequences = bsz * grpo_samples)
  - `sequence_ids`: `List[str]` # Unique IDs (len = num_sequences)
  - `group_ids`: `List[int]` # ID indicating the original prompt group for advantage calculation (len = num_sequences)
  - `policy_version`: `int` # Policy version from vLLM worker
  - `# --- TensorClass metadata ---`
  - `batch_size`: `torch.Size([num_sequences])` # Number of sequences in this output object

### 2. `TrajectoryData` (`TensorClass`)

- Purpose: Store processed data for *sequences* in the `ReplayBuffer`. Contains *unpadded* data.
- Fields:
  - `prompt_tokens`: `torch.Tensor` # Unpadded prompt, shape (prompt_len,)
  - `response_tokens`: `torch.Tensor` # Unpadded response, shape (response_len,)
  - `logprobs`: `torch.Tensor` # Unpadded response logprobs from vLLM, shape (response_len,)
  - `ref_logprobs`: `torch.Tensor` # Unpadded response ref logprobs from RefActor, shape (response_len,)
  - `rewards`: `torch.Tensor` # Shape `(num_funcs,)` per-sequence reward vector
  - `advantages`: `torch.Tensor` # Scalar per-sequence advantage, shape (1,)
  - `successes`: `torch.Tensor` # Shape `(num_funcs,)` per-sequence success vector
  - `answer`: `str` # Ground truth answer (Non-tensor field)
  - `sequence_id`: `str` # Unique ID (Non-tensor field)
  - `group_id`: `int` # Original prompt group ID (Non-tensor field)
  - `reward_metadata`: `Dict` # Metadata from reward calculation (Non-tensor field)
  - `policy_version`: `int` # Policy version at generation time (Non-tensor field)
  - `# --- TensorClass metadata ---`
  - `batch_size`: `torch.Size([1])` # TensorClass metadata indicating a single sequence

### 3. `PackedTrajectory` (`TensorClass`)

- Purpose: Represents a *packed batch* created dynamically *within* `RefActor` and `PyTorchActorModel` for processing, padded to `max_seq_len`. **Not stored in the ReplayBuffer.** Contains tensors necessary for model forward pass and potentially loss calculation.
- Fields:
  - `# --- Core Packed Tensors (Padded to max_seq_len) ---`
  - `packed_tokens`: `torch.Tensor` # Shape: `(max_seq_len,)`. Combined P+R tokens, padded.
  - `attention_mask`: `torch.Tensor` # Shape: `(max_seq_len, max_seq_len)`. Block-diagonal causal attention mask, handles padding.
  - `response_mask`: `torch.Tensor` # Shape: `(max_seq_len,)`. Boolean mask, True for actual response tokens, False elsewhere (prompts, padding).
  - `sample_mask`: `torch.Tensor` # Shape: `(max_seq_len,)`. Integer mask indicating sequence index (0 to N-1), -1 for padding.
  - `# --- Original Lengths (Used for Mask Generation, etc.) ---`
  - `prompt_lens`: `torch.Tensor` # Shape: `(num_sequences_in_pack,)`. Length of each prompt.
  - `response_lens`: `torch.Tensor` # Shape: `(num_sequences_in_pack,)`. Length of each *original* response.
  - `# --- Fields Populated *Only* in PyTorchActorModel for Loss ---`
  - `packed_ref_logprobs`: `Optional[torch.Tensor]` # Shape: `(max_seq_len,)`. Ref logprobs scattered onto response positions, padded with a placeholder (e.g., 0 or -inf). Populated during packing in `PyTorchActorModel`.
  - `packed_advantages`: `Optional[torch.Tensor]` # Shape: `(max_seq_len,)`. Advantages scattered/broadcast onto response positions, padded with a placeholder. Populated during packing in `PyTorchActorModel`.
  - `targets`: `Optional[torch.Tensor]` # Shape: `(max_seq_len,)`. Clone of `packed_tokens` but with `ignore_index` for prompts, padding, and potentially the last response token. Prepared during packing in `PyTorchActorModel`.
  - `# --- Metadata (Carried alongside the pack) ---`
  - `answers`: `List[str]` # Ground truth answers (len = num_sequences_in_pack). (Non-tensor field)
  - `sequence_ids`: `List[str]` # Unique IDs (len = num_sequences_in_pack). (Non-tensor field)
  - `group_ids`: `List[int]` # Original group IDs (len = num_sequences_in_pack). (Non-tensor field)
  - `reward_metadata`: `Dict` # Metadata from reward calculation (e.g., func_names). (Non-tensor field)
  - `policy_versions`: `List[int]` # Policy version for each sequence. (Non-tensor field)
  - `# --- TensorClass metadata ---`
  - `batch_size`: `torch.Size([max_seq_len])` # TensorClass metadata for the packed dimension

## Worker Modifications

### 1. `SyncLLMCollector`

- `_postprocess_for_queue`:
  1. Receive potentially padded `data` (TensorDict) from `self.policy` (vLLM / LLMEnv).
  2. **Extract Raw Data:** Iterate through the batch (`bsz * grpo_samples`). For each sequence `i`:
     - Determine actual prompt length (`prompt_len_i`) and response length (`response_len_i`) from masks or stop tokens.
     - Extract unpadded `prompt_tokens_i`: `data["tokens"][i, :prompt_len_i]`.
     - Extract unpadded `response_tokens_i`: `data["tokens_response"][i, :response_len_i]`.
     - Extract unpadded `logprobs_i`: `data["log_probs"][i, :response_len_i]`.
     - Store these tensors in lists: `prompts: List[torch.Tensor]`, `responses: List[torch.Tensor]`, `logprobs: List[torch.Tensor]`.
     - **Note:** For efficiency, consider using `torch.split` or `torch.narrow` to extract subtensors in bulk instead of a loop, especially for large batches.
  3. Generate `group_ids` (e.g., `[0]*G + [1]*G + ... + [B-1]*G`).
  4. Keep `answers` and `sequence_ids` into lists. Get `policy_version`.
  5. Create `VLLMOutput` instance containing the lists of unpadded tensors and metadata.
  6. Put `VLLMOutput` into `rollout_queue`.
  7. Return `total_generated_tokens` (sum of actual `response_len_i`).

### 2. `RefActor`

- `run` loop:
  1. **Dequeue & Accumulate:** Maintain a `current_batch_sequences` list (of `VLLMOutput`). Loop, getting `VLLMOutput` from `rollout_queue`. Add to list. Keep track of cumulative token count (`prompt+response`).
  2. **Check Overflow & Pack:** Before adding a new sequence, check if `cumulative_tokens + new_seq_tokens > max_seq_len`.
     - If it overflows:
       - Call `pack_sequences(current_batch_sequences, max_seq_len, ...)` to get `packed_batch: PackedTrajectory`.
       - Process `packed_batch` (steps 3-7).
       - Clear `current_batch_sequences`.
       - Start `current_batch_sequences` with the new sequence that caused the overflow. Update `cumulative_tokens`.
     - If it fits: Add the new sequence, update `cumulative_tokens`. Continue dequeuing.
  3. **Forward Pass (Ref Model):**
     - Input `packed_batch.packed_tokens` and `packed_batch.attention_mask` to `self._ref_model`. Get `ref_logits` (shape `(max_seq_len, vocab_size)`).
  4. **Calculate** `ref_logprobs` **(Unpacked):**
     - Compute sequence logprobs from `ref_logits` using `packed_batch.packed_tokens` (as targets) and `packed_batch.response_mask` to select the relevant logits. This yields packed logprobs (shape `max_seq_len`).
     - Use `extract_response_logprobs_from_pack(packed_logprobs, packed_batch.sample_mask, packed_batch.response_mask, num_sequences)` to get the per-sequence, *unpadded* `ref_logprobs` (List[Tensor]).
     - **Note:** Ensure `extract_response_logprobs_from_pack` correctly efficiently extracts unpadded logprobs for each sequence using the masks.
  5. **Compute Rewards:**
     - Retrieve the unpacked `responses` (List[Tensor]) and `answers` (List[str]) directly from the `current_batch_sequences` (list of `VLLMOutput`).
     - Call `batched_rewards(tokenizer, responses, answers, device)`. Get `rewards_by_fn`, `successes_by_fn` (shape `(num_sequences_in_pack, num_funcs)`). Store `reward_metadata`.
  6. **Compute Advantages:**
     - Sum `rewards_by_fn` to get `total_rewards` per sequence.
     - Use `packed_batch.group_ids` to group `total_rewards`. Calculate mean/std per group ID.
     - Compute `advantages` per sequence (shape `(num_sequences_in_pack,)`).
  7. **Prepare Data for Replay Buffer:**
     - Get unpacked `prompt_tokens`, `response_tokens`, `logprobs` for sequence from `current_batch_sequences`.
     - Create a single `TrajectoryData` containing all sequence with its unpacked tensors (`prompt_tokens`, `response_tokens`, `logprobs`, `ref_logprobs[i]`) and computed values (`rewards_by_fn`, `advantages`, `successes_by_fn`, `answer`, `sequence_id`, `group_id`, `reward_metadata`, `policy_version`). Add batch_size arg as len(sequences)

  8. **Add to Buffer:** `self.replay_buffer.extend(TrajectoryData)`. The buffer handles concatenating the samples of the `TrajectoryData` TensorClasses from different steps.
  9. **Logging:** Log statistics about the processed *unpacked* sequences.

### 3. `PyTorchActorModel`

- `train` loop:
  1. **Sample & Accumulate:** Maintain `current_batch_sequences` list (a `TrajectoryData` that holds List[Tensors]). Loop, sampling `TrajectoryData` instances from `replay_buffer` (e.g., `replay_buffer.sample(1)`). Add to list. Track cumulative tokens.
     - **Note:** To avoid repeat sampling, keep track of sample ids in the current batch
  2. **Check Overflow & Pack:** Before adding a new sequence, check for overflow against `max_seq_len`.
     - If overflow:
       - Call `pack_sequences(current_batch_sequences, max_seq_len, ...)` to get `packed_batch: PackedTrajectory`. Store the associated `current_batch_sequences` (list of `TrajectoryData`) alongside `packed_batch`.
       - Process this `packed_batch` and its associated unpacked data (steps 4-7).
       - Clear `current_batch_sequences`. Start new list with the overflow sequence. Update cumulative tokens.
     - If fits: Add sequence, update cumulative tokens. Continue sampling.
     - **Crucially:** During this packing step *within* `PyTorchActorModel`, populate the optional fields in `packed_batch`:
       - `packed_ref_logprobs`: Scatter `sampled_unpacked_batch.ref_logprobs` using `sample_mask` and `response_mask`.
       - `packed_advantages`: Scatter/broadcast `sampled_unpacked_batch.advantages` using `sample_mask` and `response_mask`.
       - `targets`: Create the masked targets tensor (`packed_tokens.clone()`, apply `ignore_index` using `response_mask`, potentially mask last token).
       - **Note:** Verify scattering logic ensures correct placement of values in the packed tensor.
  3. `_prepare_trajectory`: Removed. Packing handles preparation.
  4. `grpo_step`:
     - Input: `packed_batch: PackedTrajectory`. (The `sampled_unpacked_batch` is available in the outer scope if needed for metadata/logging).
     - **Forward Pass (Policy Model):**
       - Call `self._model(packed_batch.packed_tokens, packed_batch.attention_mask)`. Get `pi_logits`.
     - **Loss Calculation:**
       - Call `self._loss_fn(pi_logits=pi_logits, targets=packed_batch.targets, ref_logprobs=packed_batch.packed_ref_logprobs, advantages=packed_batch.packed_advantages, padding_masks=packed_batch.response_mask)`.
       - The loss function internally handles the masking based on `padding_masks`. Ensure `targets` is correctly pre-masked.
       - Returns `loss`, scalar stats, and `pi_logprobs` (packed shape, valid only where `response_mask` is True). Per-sequence/token stats for logging can be derived later using masks.
     - **Backward Pass:** `loss.backward()`.
     - **Return Stats:** Return `GRPOStats`. Include computed `pi_logprobs` (packed) for logging.
  5. **Optimizer Step/Zero Grad:** Standard. Handle gradient accumulation across PPO epochs if `ppo_epochs > 1`.
  6. **Logging (`_log_metrics`, `_log_debug_table`):**
     - **Metrics:** Log scalar loss stats. Calculate others using masks on packed tensors (e.g., KL divergence using `pi_logprobs`, `packed_ref_logprobs`, and `response_mask`) or from `sampled_unpacked_batch`.
     - **Debug Table:**
       1. Requires `sampled_unpacked_batch: TrajectoryData` and the `packed_batch: PackedTrajectory` (especially its masks) and computed `pi_logprobs` (packed).
       2. Iterate `i` through the sequences represented in the pack (0 to `num_sequences_in_pack - 1`).
       3. No need to decode `prompt_tokens`, `response_tokens`, since we have it from `sampled_unpacked_batch`.
       4. Use `packed_batch.sample_mask == i` and `packed_batch.response_mask` to extract the segment of `pi_logprobs` corresponding to sequence `i`'s response.
       5. Get `ref_logprobs`, `rewards`, `advantages`, etc., from `sampled_unpacked_batch` for sequence `i`.
       6. Construct table rows.

#TODO: review these functions
## Helper Functions (Python Signatures)

```python
from typing import List, Tuple, Dict, Any, Union
import torch
from torchtune.dev.rl.types import VLLMOutput, ProcessedSequenceData, PackedTrajectory # Assuming these types exist

class TrajectoryData(TensorClass): ...
class PackedTrajectory(TensorClass): ...
class VLLMOutput(TensorClass): ...

def pack_sequences(
    sequences: Union[List[VLLMOutput], List[TrajectoryData], TrajectoryData], # Input can be list or stacked TensorClass
    max_seq_len: int,
    device: torch.device,
    pad_token_id: int,
    ignore_index: int = -100, # For target masking
    populate_for_loss: bool = False, # If True, populates fields needed only by PyTorchActorModel loss
) -> PackedTrajectory:
    """
    Packs a list/batch of individual sequences into a single PackedTrajectory batch.

    This function takes a list of VLLM outputs or TrajectoryData instances,
    or a single stacked TrajectoryData batch (as sampled from the replay buffer),
    and concatenates their tokens (prompt + response) into a single tensor batch,
    padded to max_seq_len. It generates the necessary masks (response, sample,
    attention) also padded to max_seq_len.

    If `populate_for_loss` is True (used in PyTorchActorModel), it also creates
    and populates `packed_ref_logprobs`, `packed_advantages`, and `targets`
    by scattering the corresponding data from the input sequences.

    Args:
        sequences: A list of sequence data objects (VLLMOutput or TrajectoryData) OR
                   a single TrajectoryData object where fields hold stacked data for the batch.
        max_seq_len: The maximum sequence length for the packed batch. Output tensors
                     will be padded to this length.
        device: The device to create the packed tensors on.
        pad_token_id: The token ID used for padding `packed_tokens`.
        ignore_index: The value used for masking in the `targets` tensor.
        populate_for_loss: If True, computes and adds fields needed for loss calculation.

    Returns:
        PackedTrajectory: A TensorClass instance containing the packed tensors,
                          masks, lengths, and metadata, padded to max_seq_len.

    Raises:
        ValueError: If the input is empty or if any individual sequence
                    (prompt + response) exceeds max_seq_len.
    """
    # Implementation details:
    # 0. Handle input type (list vs. stacked TensorClass) to get individual sequence data.
    # 1. Handle empty input.
    # 2. Extract prompts, responses, lengths, and metadata from each sequence. Check individual seq lengths <= max_seq_len.
    # 3. Store lengths (prompt_lens, response_lens) as tensors.
    # 4. Concatenate all prompt+response tokens -> packed_tokens_unpadded.
    # 5. Calculate actual_packed_len = packed_tokens_unpadded.shape[0]. Check actual_packed_len <= max_seq_len.
    # 6. Create response_mask_unpadded (True for response), sample_mask_unpadded (0 to N-1) using lengths.
    # 7. Create attention_mask_unpadded using torchtune.generation.packed_block_causal_mask
    #    with prompt/response lengths.
    # 8. Pad core tensors:
    #    - `packed_tokens`: Pad `packed_tokens_unpadded` to `max_seq_len` using `pad_token_id`.
    #    - `response_mask`: Pad `response_mask_unpadded` to `max_seq_len` using `False`.
    #    - `sample_mask`: Pad `sample_mask_unpadded` to `max_seq_len` using `-1`.
    #    - `attention_mask`: Ensure the attention mask generated covers `max_seq_len`, handling padding appropriately (likely handled by `packed_block_causal_mask` if it supports padding).
    # 9. Collect metadata (answers, sequence_ids, group_ids, policy_versions, reward_metadata) into lists.
    # 10. If `populate_for_loss`:
    #     - Create `packed_ref_logprobs` (tensor of zeros/infs, size `max_seq_len`). Scatter unpacked `ref_logprobs` using `sample_mask` and `response_mask`.
    #     - Create `packed_advantages` (tensor of zeros, size `max_seq_len`). Scatter/broadcast unpacked `advantages` using `sample_mask` and `response_mask`.
    #     - Create `targets` (clone of `packed_tokens`). Apply `ignore_index` where `response_mask` is False. Optionally, mask the last *actual* response token for each sequence as well.
    # 11. Instantiate and return PackedTrajectory TensorClass with padded tensors,
    #     original lengths (prompt_lens, response_lens), metadata lists, and optional loss-related tensors.
    pass

# Note: Consider implementing mask generation within `pack_sequences` for efficiency instead of separate functions.
def create_response_mask(
    prompt_lens: torch.Tensor,
    response_lens: torch.Tensor,
    max_seq_len: int, # Target length including padding
    device: torch.device,
) -> torch.Tensor:
    """
    Creates a boolean mask identifying response tokens within a *padded* packed sequence.

    Args:
        prompt_lens: Tensor of shape (num_sequences,) containing the length of each prompt.
        response_lens: Tensor of shape (num_sequences,) containing the length of each response.
        max_seq_len: The target total length of the packed sequence *after padding*.
        device: The device for the output tensor.

    Returns:
        torch.Tensor: Boolean tensor of shape (max_seq_len,) where True indicates
                      an actual response token and False indicates a prompt or padding token.
    """
    # Implementation details:
    # 1. Calculate total sequence lengths (prompt+response).
    # 2. Calculate cumulative sequence start indices.
    # 3. Create a base mask of False (size `max_seq_len`).
    # 4. Iterate through sequences: For sequence `i`, set mask range
    #    `[cum_start[i] + prompt_lens[i] : cum_start[i] + prompt_lens[i] + response_lens[i]]` to True.
    #    (Requires careful index handling).
    # Alternative: Generate unpadded mask first, then pad to max_seq_len.
    pass

def create_sample_mask(
    prompt_lens: torch.Tensor,
    response_lens: torch.Tensor,
    max_seq_len: int, # Target length including padding
    device: torch.device,
) -> torch.Tensor:
    """
    Creates an integer mask identifying which sequence each token belongs to in a *padded* packed sequence.

    Args:
        prompt_lens: Tensor of shape (num_sequences,) containing the length of each prompt.
        response_lens: Tensor of shape (num_sequences,) containing the length of each response.
        max_seq_len: The target total length of the packed sequence *after padding*.
        device: The device for the output tensor.

    Returns:
        torch.Tensor: Integer tensor of shape (max_seq_len,) where values range
                      from 0 to num_sequences-1 for actual tokens, and -1 for padding tokens.
    """
    # Implementation details:
    # 1. Calculate total length for each sequence (prompt+response).
    # 2. Create sequence indices tensor: `torch.arange(num_sequences, device=device)`.
    # 3. Generate unpadded mask using `indices.repeat_interleave(total_lengths)`.
    # 4. Pad the unpadded mask to `max_seq_len` with value -1.
    pass

## Open Questions / TODOs

*   **Attention Mask Function:** Verify `torchtune.generation.packed_block_causal_mask` can handle padding to `max_seq_len` correctly based on input lengths.
*   **Advantage Calculation:** Confirm advantage calculation logic using `group_ids` on unpacked data in `RefActor` is robust.
*   **TensorClass Implementation:** Define the `VLLMOutput`, `TrajectoryData` and `PackedTrajectory` TensorClasses precisely.
*   **Loss Function Input:** Double-check `GRPOWithChunkedOutputLoss` requirements regarding `targets` masking (esp. last token) and `padding_masks`. Ensure alignment with `PackedTrajectory` preparation in `PyTorchActorModel`. Confirm loss works correctly with packed, padded inputs.
*   **Multiturn Dialog:** This plan is for single-turn. Multiturn requires separate design.
*   **Padding Value for Sample Mask:** Using -1 for padding in `sample_mask` is confirmed.
*   **Performance Optimization:** Revisit storing/recomputing the attention mask later if memory is constrained.
*   **Helper Function Implementation:** Implement the Python helper functions (`pack_sequences`, mask creation/extraction, etc.). Consider implementing mask generation within `pack_sequences`.
*   **PyTorchActorModel Sampling:** Address potential repeat sampling issue if `replay_buffer.sample()` is called multiple times naively instead of sampling a batch. Ensure `replay_buffer.sample(batch_size)` behaves as expected.
*   **Final Target Masking:** Decide on the exact strategy for masking the final token in `targets` within `PyTorchActorModel`'s packing step.
*   **Liger Loss Compatibility:** Keep in mind the potential need for the loss function to also work with non-packed inputs for other use cases (like Liger), although this plan focuses on packed.
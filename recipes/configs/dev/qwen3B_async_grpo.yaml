# Config for async multi-node GRPO in dev/async_grpo.py
# using a Qwen-3B Base model (which is known to train quickly on this task).
#
# This config assumes that you've run the following command before launching
# this run:
#   tune download Qwen/Qwen2.5-3B --output-dir /tmp/Qwen2.5-3B --ignore-patterns "original/consolidated.00.pth"
#
# This config is meant to run on a whole node with 8 GPUs. You can tweak the number of workers here to change it
# but note that some workers have currently been hardcoded in the prototype and we haven't yet tested on different
# numbers of workers.

# To launch, run the following:
#     tune run dev/async_grpo --config recipes/configs/dev/qwen3B_async_grpo.yaml

# Note that unlike in synchronous recipes (all the existing ones), we don't follow a SPMD model here so passing a flag like `--nproc-per-node 8`
# won't help here. This is instead launching just the controller, which is a CPU process. The controller will in turn launch every other worker.

name: grpo_async_qwen3b
output_dir: /tmp/checkpoints/${name}
base_model_path: /tmp/Qwen2.5-3B  # Use this to train from the slightly trained SFT model

# --------- Model arguments (Using Qwen2.5-3B as example) ---------- #
model:
  _component_: torchtune.models.qwen2_5.qwen2_5_3b
tokenizer:
  _component_: torchtune.models.qwen2_5.qwen2_5_tokenizer
  path: ${base_model_path}/vocab.json
  merges_file: ${base_model_path}/merges.txt
  max_seq_len: 1024

# Weights for training the reasoning model
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: ${base_model_path}
  checkpoint_files: [
    model-00001-of-00002.safetensors,
    model-00002-of-00002.safetensors,
  ]
  recipe_checkpoint: null
  output_dir: ${output_dir}
  model_type: QWEN2
num_trainer_workers: 2
resume_from_checkpoint: False

# Weights for reference model
ref_checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: ${base_model_path}
  checkpoint_files: [
    model-00001-of-00002.safetensors,
    model-00002-of-00002.safetensors,
  ]
  model_type: QWEN2


resume_from_checkpoint: False
save_every_n_epochs: 1

# Fine-tuning arguments
batch_size: 16
grpo_samples: 16
max_generated_tokens: 512
top_k: null
temperature: 1.0
replay_buffer_size: ${total_inference_batch_size}  # TODO this needs to be fixed. Right now this can't be bigger, or else we'll get padding issues

ppo_epochs: 1

num_steps: 250

clip_grad_norm: 1.0

epochs: 10
optimizer:
  _component_: torch.optim.AdamW
  lr: 1e-5
  fused: True
lr_scheduler:
  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
  num_warmup_steps: 50
loss:
  _component_: torchtune.dev.grpo.loss.GRPOWithChunkedOutputLoss
  kl_coeff: 0.01
  epsilon: 0.2

# Training env
device: cuda

# Memory management
enable_activation_checkpointing: True  # True reduces memory
enable_activation_offloading: True  # True reduces memory.
compile: False  # pytorch compile, set to true for better perf/memory

# Reduced precision
dtype: bf16

# number of train_steps after which to do weight sync
steps_before_sync: 2
num_ref_workers: 1
num_fsdp_workers: 2

vllm:
  num_workers: 4
  tp_size: 1
  batch_size: 1
  steps_before_sync: 1
  queue_maxsize: ${eval:'${vllm.num_workers} * ${steps_before_sync}'}


# Logging
metric_logger:
  _component_: torchtune.training.metric_logging.WandBLogger
  log_dir: ${output_dir}/logs

log_every_n_steps: 1
log_peak_memory_stats: True

debug_logging_enabled: False
debug_num_samples_per_step: 1

# Useful for understanding how to optimize memory and performance
profiler:
  _component_: torchtune.training.setup_torch_profiler
  enabled: False

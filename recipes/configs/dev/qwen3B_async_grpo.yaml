# Config for async multi-node GRPO in dev/async_grpo.py
# using a Qwen-3B Base model (which is known to train quickly on this task).
#
# This config assumes that you've run the following command before launching
# this run:
#   tune download Qwen/Qwen2.5-3B --output-dir /tmp/Qwen2.5-3B --ignore-patterns "original/consolidated.00.pth"
#
# This config is meant to run on a whole node with 8 GPUs. You can tweak the number of workers here to change it
# but note that some workers have currently been hardcoded in the prototype and we haven't yet tested on different
# numbers of workers.

# To launch, run the following:
#     tune run dev/async_grpo --config recipes/configs/dev/qwen3B_async_grpo.yaml

# Note that unlike in synchronous recipes (all the existing ones), we don't follow a SPMD model here so passing a flag like `--nproc-per-node 8`
# won't help here. This is instead launching just the controller, which is a CPU process. The controller will in turn launch every other worker.

name: grpo_async_qwen3b
output_dir: /tmp/checkpoints/${name}
base_model_path: /tmp/Qwen2.5-3B  # Use this to train from the slightly trained SFT model

# --------- Model arguments (Using Qwen2.5-3B as example) ---------- #
model:
  _component_: torchtune.models.qwen2_5.qwen2_5_3b
tokenizer:
  _component_: torchtune.models.qwen2_5.qwen2_5_tokenizer
  path: ${base_model_path}/vocab.json
  merges_file: ${base_model_path}/merges.txt
  max_seq_len: 1024

# Weights for training the reasoning model
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: ${base_model_path}
  checkpoint_files: [
    model-00001-of-00002.safetensors,
    model-00002-of-00002.safetensors,
  ]
  recipe_checkpoint: null
  output_dir: ${output_dir}
  model_type: QWEN2
num_trainer_workers: 2
resume_from_checkpoint: False

# Weights for reference model
ref_checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: ${base_model_path}
  checkpoint_files: [
    model-00001-of-00002.safetensors,
    model-00002-of-00002.safetensors,
  ]
  model_type: QWEN2
num_ref_workers: 1

# Weights for rollout model (via vLLM)
rollout_model_dir: ${base_model_path}  # Can also be any model weights on the HF Hub
num_rollout_workers: 4
rollout_tensor_parallel_dim: 1

# ---------- Data --------- #
dataset:
  _component_: torchtune.dev.grpo.gsm8k.gsm8k_dataset
  partition: 1-9/10
seed: null
shuffle: False

epochs: 10
ppo_epochs: 1

num_steps: 250
batch_size: 16
total_inference_batch_size: 16 # rollout_batch_size * grpo_samples
grpo_samples: 16

replay_buffer_size: ${total_inference_batch_size}  # TODO this needs to be fixed. Right now this can't be bigger, or else we'll get padding issues

rollout_batch_size: 1
rollout_queue_maxsize: 4 # num_rollout_workers * num_rollouts_before_weight_update

max_generated_tokens: 512
top_k: null
temperature: 1.0

# --------- Training arguments ------------ #
optimizer:
  _component_: torch.optim.AdamW
  lr: 1e-5
  fused: True

lr_scheduler:
  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
  num_warmup_steps: 50

loss:
  _component_: torchtune.dev.grpo.loss.GRPOWithChunkedOutputLoss
  kl_coeff: 0.01
  epsilon: 0.2

num_train_steps_before_sync: 2
num_rollouts_before_weight_update: 1

clip_grad_norm: 1.0

save_every_n_steps: 200

# Memory management / performance
enable_activation_checkpointing: True  # True reduces memory
enable_activation_offloading: True  # True reduces memory.
compile: False  # pytorch compile, set to true for better perf/memory

# ---------- Logging ------------ #
metric_logger:
  _component_: torchtune.training.metric_logging.WandBLogger
  log_dir: ${output_dir}/logs
log_every_n_steps: 1
log_peak_memory_stats: True

debug_logging_enabled: False
debug_num_samples_per_step: 1

# Useful for understanding how to optimize memory and performance
profiler:
  _component_: torchtune.training.setup_torch_profiler
  enabled: False

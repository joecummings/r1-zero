# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree.


from dataclasses import dataclass
import torch
from typing import List, Dict, Optional, Any

@dataclass
class UnscoredTrajectory:
    """
    Represents a single trajectory generated by the policy model before scoring.

    Attributes:
        prompt_tokens (torch.Tensor): Unpadded prompt tokens. Shape: (prompt_len,).
        response_tokens (torch.Tensor): Unpadded response tokens generated by the policy.
            Shape: (response_len,).
        prompt_len (int): Length of the prompt tokens.
        response_len (int): Length of the response tokens.
        logprobs (torch.Tensor): Log probabilities of the response tokens from the
            generating policy (e.g., vLLM). Shape: (response_len,).
        answer (str): The ground truth or reference answer associated with the prompt.
        sequence_id (str): A unique identifier for this specific trajectory.
        policy_version (int): The version identifier of the policy model that generated
            this trajectory.
        group_id (str): An identifier grouping trajectories generated from the same base
            prompt, used for advantage computation.
    """
    prompt_tokens: torch.Tensor
    response_tokens: torch.Tensor
    prompt_len: int
    response_len: int
    logprobs: torch.Tensor
    answer: str
    sequence_id: str
    policy_version: int
    group_id: str

@dataclass
class GroupedUnscoredTrajectories:
    """
    Represents a collection of unscored trajectories generated from the same base prompt.

    Attributes:
        trajectories (List[UnscoredTrajectory]): A list of individual unscored trajectories
            belonging to this group.
        group_id (str): The unique identifier shared by all trajectories in this group.
        total_tokens (int): The sum of prompt lengths and response lengths for all
            trajectories in the group.
    """
    trajectories: List[UnscoredTrajectory]
    group_id: str
    total_tokens: int

@dataclass
class ScoredTrajectory:
    """
    Represents a single trajectory after it has been scored by reward functions
    and a reference model.

    Attributes:
        prompt_tokens (torch.Tensor): Unpadded prompt tokens. Shape: (prompt_len,).
        response_tokens (torch.Tensor): Unpadded response tokens generated by the policy.
            Shape: (response_len,).
        prompt_len (int): Length of the prompt tokens.
        response_len (int): Length of the response tokens.
        logprobs (torch.Tensor): Log probabilities of the response tokens from the
            generating policy (e.g., vLLM). Shape: (response_len,).
        ref_logprobs (torch.Tensor): Log probabilities of the response tokens from the
            reference model. Shape: (response_len,).
        rewards (torch.Tensor): Reward values assigned by different reward functions.
            Shape: (num_funcs,).
        advantages (torch.Tensor): The calculated advantage estimate for this trajectory
            (scalar).
        successes (torch.Tensor): Success indicators from different reward functions.
            Shape: (num_funcs,).
        answer (str): The ground truth or reference answer associated with the prompt.
        sequence_id (str): A unique identifier for this specific trajectory.
        group_id (str): An identifier grouping trajectories generated from the same base
            prompt.
        reward_metadata (Dict[str, Any]): Additional metadata returned by the reward
            computation process (e.g., names of reward functions).
        policy_version (int): The version identifier of the policy model that generated
            this trajectory.
    """
    prompt_tokens: torch.Tensor
    response_tokens: torch.Tensor
    prompt_len: int
    response_len: int
    logprobs: torch.Tensor
    ref_logprobs: torch.Tensor
    rewards: torch.Tensor
    advantages: torch.Tensor
    successes: torch.Tensor
    answer: str
    sequence_id: str
    group_id: str
    reward_metadata: Dict[str, Any]
    policy_version: int

    def to(self, device: torch.device) -> 'ScoredTrajectory':
        """Move all tensors to the specified device."""
        return ScoredTrajectory(
            prompt_tokens=self.prompt_tokens.to(device),
            response_tokens=self.response_tokens.to(device),
            prompt_len=self.prompt_len,
            response_len=self.response_len,
            logprobs=self.logprobs.to(device),
            ref_logprobs=self.ref_logprobs.to(device),
            rewards=self.rewards.to(device),
            advantages=self.advantages.to(device),
            successes=self.successes.to(device),
            answer=self.answer,
            sequence_id=self.sequence_id,
            group_id=self.group_id,
            reward_metadata=self.reward_metadata,
            policy_version=self.policy_version
        )

@dataclass
class PackedTrajectory:
    """
    Represents a batch of trajectories packed into single tensors for efficient
    model processing.

    Attributes:
        tokens (torch.Tensor): Concatenated prompt and response tokens for all sequences
            in the batch, potentially padded. Shape: (packed_seq_len,).
            Example: [P1, R1, P2, R2, ...].
        attention_mask (torch.Tensor): Block-diagonal causal attention mask for the
            packed sequences. Shape: (packed_seq_len, packed_seq_len).
        position_ids (torch.Tensor): Position IDs for the packed sequences. Shape: (packed_seq_len,).
        response_mask (torch.Tensor): Boolean mask indicating the positions of response
            tokens within the `tokens` tensor. Shape: (packed_seq_len,).
        sequence_mask (torch.Tensor): Integer mask indicating which sequence each token
            belongs to. Shape: (packed_seq_len,).
        prompt_lens (torch.Tensor): Lengths of the prompts for each sequence in the batch.
            Shape: (num_sequences,).
        response_lens (torch.Tensor): Lengths of the responses for each sequence in the
            batch. Shape: (num_sequences,).
        sequence_map (torch.Tensor): Start and end indices for each sequence within the
            `tokens` tensor. Shape: (num_sequences, 2).
        packed_seq_len (int): The total length of the `tokens` tensor, including any
            padding.
        ref_logprobs (Optional[torch.Tensor]): Concatenated reference model log
            probabilities for all response tokens. Shape: (total_response_tokens,).
        advantages (Optional[torch.Tensor]): Concatenated advantage values for all
            response tokens. Shape: (total_response_tokens,).
        targets (Optional[torch.Tensor]): Concatenated target tokens (usually the
            response tokens shifted). Shape: (total_response_tokens,).
        sequence_ids (Optional[List[str]]): List of unique sequence identifiers
            corresponding to the packed sequences.
        group_ids (Optional[List[str]]): List of group identifiers corresponding to the
            packed sequences.
        actual_total_tokens (int): The total number of tokens in the batch, excluding
            any padding tokens used in `tokens`.
    """
    tokens: torch.Tensor
    attention_mask: torch.Tensor
    position_ids: torch.Tensor
    response_mask: torch.Tensor
    sequence_mask: torch.Tensor
    prompt_lens: torch.Tensor
    response_lens: torch.Tensor
    sequence_map: torch.Tensor
    packed_seq_len: int
    ref_logprobs: Optional[torch.Tensor] = None
    advantages: Optional[torch.Tensor] = None
    targets: Optional[torch.Tensor] = None
    sequence_ids: Optional[List[str]] = None
    group_ids: Optional[List[str]] = None
    actual_total_tokens: int = 0

@dataclass
class GRPOStats:
    """
    Contains statistics computed during a GRPO (Generalized Reward Policy Optimization)
    training step.

    Attributes:
        loss (torch.Tensor): The total scalar loss for the GRPO step.
        policy_loss (torch.Tensor): The scalar component of the loss related to the
            policy objective.
        kl_loss (torch.Tensor): The scalar component of the loss penalizing KL divergence
            from the reference policy.
        ratios (torch.Tensor): The scalar mean of the importance sampling ratios
            (pi_theta / pi_ref).
        clipfrac (torch.Tensor): The scalar fraction of ratios that were clipped
            according to the PPO clipping mechanism.
        approx_policy_kls (torch.Tensor): A scalar estimate of the KL divergence between
            the policy before and after the optimization step.
        metadata (Optional[Dict[str, Any]]): A dictionary containing additional data for
            debugging or logging purposes (e.g., policy log probabilities `pi_logprobs`).
    """
    loss: torch.Tensor
    policy_loss: torch.Tensor
    kl_loss: torch.Tensor
    ratios: torch.Tensor
    clipfrac: torch.Tensor
    approx_policy_kls: torch.Tensor
    metadata: Optional[Dict[str, Any]] = None


class GRPOTrajectory(NamedTuple):
    """
    Contains a collection of tensors describing a generated trajectory during GRPO training.

    Attributes:
        query_responses (torch.Tensor): (query, response) pairs with shape [B x G, P+L].
        logprobs (torch.Tensor): Log probabilities of the generated responses with shape [B x G, L].
        ref_logprobs (torch.Tensor): Log probabilities of the generated responses using the reference policy with shape [B x G, L].
        advantages (torch.Tensor): Advantage estimates for the generated responses with shape [B x G].
        masks (torch.Tensor): Attention masks for input ids-generated responses pairs with shape [B x G, P+L, P+L].
        position_ids (torch.Tensor): Position IDs for input ids-generated responses pairs with shape [B x G, P+L].
        response_padding_masks (torch.Tensor): Padding masks for the truncated and padded generated responses with shape [B x G, L].
        seq_lens (torch.Tensor): Sequence lengths of truncated generated responses.
    """

    query_responses: torch.Tensor = None  # [B x G, P+L]
    logprobs: torch.Tensor = None  # [B x G, L]
    ref_logprobs: torch.Tensor = None  # [B x G, L]
    advantages: torch.Tensor = None  # [B x G]
    masks: torch.Tensor = None  # [B x G, P+L, P+L]
    position_ids: torch.Tensor = None  # [B x G, P+L]
    response_padding_masks: torch.Tensor = None  # [B x G, L]
    seq_lens: torch.Tensor = None


